{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, OPTForCausalLM\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import random\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def set_random_seed(seed: int):\n",
    "    print(\"Seed: {}\".format(seed))\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_random_seed(1)\n",
    "\n",
    "print(\"starting\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "file_path = 'dataset.xlsx'  \n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(data)\n",
    "train_testval = hf_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "test_val = train_testval['test'].train_test_split(test_size=2/3, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testval['train'],\n",
    "    'validation': test_val['train'],\n",
    "    'test': test_val['test']\n",
    "})\n",
    "\n",
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, model_max_length = 512, truncation_side = 'left')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model=model.to(device)\n",
    "\n",
    " \n",
    "max_input_length = 1024\n",
    "max_target_length = 1024\n",
    "conversation_history = {}\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    global conversation_history\n",
    "\n",
    "    user_ids = examples[\"ID\"]\n",
    "    user_texts = [text if isinstance(text, str) else \"\" for text in examples[\"User\"]]\n",
    "    bot_texts = [text if isinstance(text, str) else \"\" for text in examples[\"BOT\"]]\n",
    "    best_contexts = [text if isinstance(text, str) else \"\" for text in examples[\"BestContextValue\"]]\n",
    "\n",
    "    updated_inputs = []\n",
    "\n",
    "    for user_id, user_text, best_context in zip(user_ids, user_texts, best_contexts):\n",
    "        if user_id not in conversation_history:\n",
    "            conversation_history[user_id] = \"\"\n",
    "\n",
    "        historical_context = conversation_history[user_id]\n",
    "        updated_input = f\"Context: {best_context} History: {historical_context} User: {user_text}\"\n",
    "        updated_inputs.append(updated_input)\n",
    "\n",
    "        if bot_texts:\n",
    "            conversation_history[user_id] += f\" User: {user_text} Bot: {bot_texts[user_ids.index(user_id)]}\"\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        updated_inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        bot_texts,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_sequence]\n",
    "        for label_sequence in labels\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "class FinDataset(Dataset):\n",
    "\n",
    "    def _init_(self, input_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx]\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(dataset[\"train\"].column_names)\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"{split} Input IDs Shape: {len(tokenized_datasets[split]['input_ids'][0])}\")\n",
    "    print(f\"{split} Labels Shape: {len(tokenized_datasets[split]['labels'][0])}\")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model)\n",
    "\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load('bleu')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "meteor = evaluate.load('meteor')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_result = rouge_score.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    bleu_result = bleu.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    bleu_scores = {f\"bleu{i+1}\": round(score, 4) for i, score in enumerate(bleu_result['precisions'])}\n",
    "\n",
    "    bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "\n",
    "    meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    rouge_result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    metrics = {\n",
    "        \"rouge\": {k: round(v, 4) for k, v in rouge_result.items()},\n",
    "        **bleu_scores,\n",
    "        \"meteor\": round(meteor_result[\"meteor\"], 4),\n",
    "        \"bertscore_precision\": round(np.mean(bertscore_result['precision']), 4),\n",
    "        \"bertscore_recall\": round(np.mean(bertscore_result['recall']), 4),\n",
    "        \"bertscore_f1\": round(np.mean(bertscore_result['f1']), 4),\n",
    "        \"gen_len\": round(np.mean(prediction_lens), 4)\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login('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')\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"vicuna-context\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy='epoch',\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finbot3] *",
   "language": "python",
   "name": "conda-env-finbot3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
