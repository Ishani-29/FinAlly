{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "from rouge_score.rouge_scorer import RougeScorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from evaluate import load\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "bertscore = load(\"bertscore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfaa0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed: int):\n",
    "    print(\"Seed: {}\".format(seed))\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_random_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5459c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, model_max_length = 512, truncation_side = 'left')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model=model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f07af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_excel_data_grouped(excel_path: str):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    print(\"Columns in the Excel file:\", df.columns)\n",
    "    \n",
    "    if 'dialogue_id' not in df.columns:\n",
    "        raise KeyError(\"'dialogue_id' column is missing in the provided Excel file.\")\n",
    "    \n",
    "    grouped_dialogues = []\n",
    "    grouped = df.groupby('dialogue_id')\n",
    "\n",
    "    for dialogue_id, group in grouped:\n",
    "        user_inputs = group['User'].tolist()\n",
    "        bot_responses = group['BOT'].tolist()\n",
    "        combined_input = \"\\n\".join([f\"User: {u}\" for u in user_inputs])\n",
    "        combined_reference = \"\\n\".join([f\"Bot: {b}\" for b in bot_responses])\n",
    "        \n",
    "        grouped_dialogues.append({\n",
    "            \"dialogue_id\": dialogue_id,\n",
    "            \"combined_user\": combined_input,\n",
    "            \"combined_bot\": combined_reference\n",
    "        })\n",
    "    \n",
    "    return grouped_dialogues\n",
    "\n",
    "dialogues = load_excel_data_grouped('/mnt/Data/sarmistha/Financial Chatbot/Ecosage_Correct_Code/data single sheet.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ee871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def generate_responses_step_by_step(dialogues):\n",
    "    \"\"\"\n",
    "    Generate responses step-by-step for each grouped conversation.\n",
    "    \"\"\"\n",
    "    all_generated_responses = []\n",
    "\n",
    "    for idx, dialogue in enumerate(dialogues):\n",
    "        print(f\"Processing Dialogue ID: {dialogue['dialogue_id']} ({idx+1}/{len(dialogues)})\")\n",
    "        conversation_history = \"\"  \n",
    "        generated_responses = []   \n",
    "\n",
    "        user_inputs = dialogue['combined_user'].split('\\n')\n",
    "        for turn, user_input in enumerate(user_inputs):\n",
    "            current_input = conversation_history + f\"User: {user_input}\\nBot:\"\n",
    "\n",
    "            inputs = tokenizer(current_input, return_tensors='pt', truncation=True).to(device)\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=100, \n",
    "                do_sample=True, \n",
    "                temperature=0.7, \n",
    "                top_p=0.7, \n",
    "                top_k=50, \n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "\n",
    "            generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "            bot_response = generated_text[len(current_input):].strip().split('\\n')[0]\n",
    "            print(f\"User: {user_input}\")\n",
    "            print(f\"Generated Bot: {bot_response}\\n\")\n",
    "\n",
    "            conversation_history += f\"User: {user_input}\\nBot: {bot_response}\\n\"\n",
    "\n",
    "            generated_responses.append(bot_response)\n",
    "\n",
    "        all_generated_responses.append({\n",
    "            \"dialogue_id\": dialogue['dialogue_id'],\n",
    "            \"generated_responses\": generated_responses\n",
    "        })\n",
    "\n",
    "    return all_generated_responses\n",
    "\n",
    "generated_responses = generate_responses_step_by_step(dialogues)\n",
    "\n",
    "response_data = []\n",
    "\n",
    "for result in generated_responses:\n",
    "    dialogue_id = result[\"dialogue_id\"]\n",
    "    for bot_response in result[\"generated_responses\"]:\n",
    "        response_data.append({\n",
    "            \"dialogue_id\": dialogue_id,\n",
    "            \"generated_response\": bot_response\n",
    "        })\n",
    "\n",
    "df_responses = pd.DataFrame(response_data)\n",
    "\n",
    "os.makedirs('./generated_responses', exist_ok=True)\n",
    "\n",
    "csv_file_path = './generated_responses/step_by_step_generated_responses.csv'\n",
    "df_responses.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"\\nGeneration complete. Responses saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(reference_list: list, hypothesis_list: list):\n",
    "    count = 0\n",
    "    rouge1 = 0\n",
    "    rouge2 = 0\n",
    "    rougel = 0\n",
    "    bleu_1 = 0\n",
    "    bleu_2 = 0\n",
    "    bleu_3 = 0\n",
    "    bleu_4 = 0\n",
    "    rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "    bert_precision_score = 0.0\n",
    "    bert_recall_score = 0.0\n",
    "    bert_f1_score = 0.0\n",
    "\n",
    "    for reference, hypothesis in zip(reference_list, hypothesis_list):\n",
    "        scores = rouge_scorer.score(reference, hypothesis)\n",
    "        rouge1 += scores['rouge1'].fmeasure\n",
    "        rouge2 += scores['rouge2'].fmeasure\n",
    "        rougel += scores['rougeL'].fmeasure\n",
    "\n",
    "        bert_results = bertscore.compute(predictions=[hypothesis], references=[reference], model_type='microsoft/deberta-xlarge-mnli')\n",
    "        bert_precision = bert_results['precision']\n",
    "        bert_recall = bert_results['recall']\n",
    "        bert_f1 = bert_results['f1']\n",
    "\n",
    "        bert_precision_score += sum(bert_precision)\n",
    "        bert_recall_score += sum(bert_recall)\n",
    "        bert_f1_score += sum(bert_f1)\n",
    "\n",
    "        reference = reference.split()\n",
    "        hypothesis = hypothesis.split()\n",
    "        bleu_1 += sentence_bleu([reference], hypothesis, weights=(1.,))\n",
    "        bleu_2 += sentence_bleu([reference], hypothesis, weights=(1./2., 1./2.))\n",
    "        bleu_3 += sentence_bleu([reference], hypothesis, weights=(1./3., 1./3., 1./3.))\n",
    "        bleu_4 += sentence_bleu([reference], hypothesis, weights=(1./4., 1./4., 1./4., 1./4.))\n",
    "        count += 1\n",
    "\n",
    "    return {\n",
    "        \"rouge_1\": rouge1 * 100 / count,\n",
    "        \"rouge_2\": rouge2 * 100 / count,\n",
    "        \"rouge_L\": rougel * 100 / count,\n",
    "        \"bleu_1\": bleu_1 * 100 / count,\n",
    "        \"bleu_2\": bleu_2 * 100 / count,\n",
    "        \"bleu_3\": bleu_3 * 100 / count,\n",
    "        \"bleu_4\": bleu_4 * 100 / count,\n",
    "        'bert_precision': bert_precision_score * 100 / count,\n",
    "        'bert_recall': bert_recall_score * 100 / count,\n",
    "        'bert_f1': bert_f1_score * 100 / count\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [\n",
    "    line.strip() for dialogue in dialogues for line in dialogue['combined_bot'].split(\"\\nBot:\") if line.strip()\n",
    "]\n",
    "hypotheses = [\n",
    "    line.strip() for generated in generated_responses for line in generated['generated_responses'] if line.strip()\n",
    "]\n",
    "\n",
    "metrics = get_scores(references, hypotheses)\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finbot3] *",
   "language": "python",
   "name": "conda-env-finbot3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
